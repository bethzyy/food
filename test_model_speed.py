#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GLMæ¨¡å‹é€Ÿåº¦å¯¹æ¯”æµ‹è¯•è„šæœ¬
æµ‹è¯• GLM-4-flash å’Œ GLM-4.7 çš„å“åº”é€Ÿåº¦
"""

import os
import sys
import time
import json
import statistics
from typing import Dict, List, Tuple
import requests

# è®¾ç½®UTF-8ç¼–ç 
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# æµ‹è¯•é…ç½®
API_URL = "https://open.bigmodel.cn/api/anthropic/v1/messages"
TEST_COUNT = 3  # æ¯ä¸ªæ¨¡å‹æµ‹è¯•æ¬¡æ•°

# æµ‹è¯•æç¤ºè¯
TEST_PROMPTS = {
    "ç®€å•": "è¯·å›ç­”ï¼š1+1ç­‰äºå‡ ï¼Ÿåªå›å¤æ•°å­—ã€‚",
    "ä¸­ç­‰": """è¯·è¯¦ç»†è§£é‡Šä»¥ä¸‹æ¦‚å¿µï¼š
1. ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ
2. æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
3. ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†ã€‚

è¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ï¼Œæ¯ä¸ªæ¦‚å¿µä¸è¶…è¿‡50å­—ã€‚""",
    "å¤æ‚": """ä½œä¸ºä¸€åä¸“ä¸šçš„è¥å…»å¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æä¾›è¯¦ç»†çš„é¥®é£Ÿå»ºè®®ï¼š

å®¢æˆ·ä¿¡æ¯ï¼š
- å¹´é¾„ï¼š35å²
- æ€§åˆ«ï¼šå¥³
- èŒä¸šï¼šåŠå…¬å®¤èŒå‘˜
- è¿åŠ¨é‡ï¼šæ¯å‘¨2-3æ¬¡è½»åº¦è¿åŠ¨
- å¥åº·ç›®æ ‡ï¼šæ”¹å–„æ¶ˆåŒ–ç³»ç»Ÿå¥åº·

è¯·æä¾›ï¼š
1. æ¯æ—¥çƒ­é‡éœ€æ±‚è®¡ç®—
2. ä¸‰å¤§è¥å…»ç´ ï¼ˆè›‹ç™½è´¨ã€è„‚è‚ªã€ç¢³æ°´ï¼‰çš„æ¨èæ‘„å…¥æ¯”ä¾‹
3. ä¸€æ—¥ä¸‰é¤çš„å…·ä½“å»ºè®®ï¼ˆåŒ…æ‹¬é£Ÿæå’Œä»½é‡ï¼‰
4. éœ€è¦é¿å…çš„é£Ÿç‰©
5. æ¨èçš„å¥åº·é›¶é£Ÿ

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ã€‚"""
}


def get_api_key() -> str:
    """è·å–API Key"""
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–
    api_key = os.environ.get("ZHIPU_API_KEY")
    if api_key:
        return api_key

    # å°è¯•ä».envæ–‡ä»¶è¯»å–
    env_file = os.path.join(os.path.dirname(__file__), "config.env")
    if os.path.exists(env_file):
        with open(env_file, "r", encoding="utf-8") as f:
            for line in f:
                if line.startswith("ZHIPU_API_KEY="):
                    return line.split("=", 1)[1].strip()

    raise ValueError("æœªæ‰¾åˆ°API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY æˆ–åœ¨ config.env ä¸­é…ç½®")


def test_model(model: str, prompt: str, max_tokens: int = 1000) -> Tuple[int, str, bool]:
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å“åº”æ—¶é—´

    Args:
        model: æ¨¡å‹åç§° (glm-4-flash æˆ– glm-4.7)
        prompt: æµ‹è¯•æç¤ºè¯
        max_tokens: æœ€å¤§tokenæ•°

    Returns:
        (å“åº”æ—¶é—´ms, è¿”å›å†…å®¹, æ˜¯å¦æˆåŠŸ)
    """
    api_key = get_api_key()

    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key
    }

    payload = {
        "model": model,
        "max_tokens": max_tokens,
        "messages": [{
            "role": "user",
            "content": prompt
        }]
    }

    try:
        start_time = time.time()
        response = requests.post(API_URL, headers=headers, json=payload, timeout=120)
        elapsed_time = int((time.time() - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        if response.status_code == 200:
            data = response.json()
            content = data.get("content", [{}])[0].get("text", "")
            return elapsed_time, content, True
        else:
            error_msg = response.json().get("error", {}).get("message", f"HTTP {response.status_code}")
            return elapsed_time, error_msg, False

    except Exception as e:
        return 0, str(e), False


def run_comprehensive_test():
    """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•"""
    print("=" * 80)
    print("[TEST] GLM Model Speed Comparison")
    print("=" * 80)
    print()

    # è·å–API Key
    try:
        api_key = get_api_key()
        print(f"âœ“ API Key: {api_key[:10]}...{api_key[-4:]}")
        print()
    except ValueError as e:
        print(f"âŒ é”™è¯¯: {e}")
        return

    # æ¨¡å‹åˆ—è¡¨
    models = [
        {"name": "GLM-4-flash", "id": "glm-4-flash", "emoji": "âš¡"},
        {"name": "GLM-4.7", "id": "glm-4.7", "emoji": "ğŸ§ "}
    ]

    # æµ‹è¯•é…ç½®
    test_configs = [
        {"complexity": "ç®€å•", "prompt": TEST_PROMPTS["ç®€å•"], "max_tokens": 500},
        {"complexity": "ä¸­ç­‰", "prompt": TEST_PROMPTS["ä¸­ç­‰"], "max_tokens": 1000},
        {"complexity": "å¤æ‚", "prompt": TEST_PROMPTS["å¤æ‚"], "max_tokens": 2000}
    ]

    # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
    all_results = {}

    for config in test_configs:
        complexity = config["complexity"]
        prompt = config["prompt"]
        max_tokens = config["max_tokens"]

        print("=" * 80)
        print(f"ğŸ“ æµ‹è¯•åœºæ™¯: {complexity}æç¤ºè¯ ({len(prompt)}å­—ç¬¦, max_tokens={max_tokens})")
        print("=" * 80)
        print()

        scenario_results = {}

        for model_info in models:
            model_name = model_info["name"]
            model_id = model_info["id"]
            emoji = model_info["emoji"]

            print(f"{emoji} æµ‹è¯• {model_name}...")
            times = []
            success_count = 0

            for i in range(TEST_COUNT):
                print(f"  ç¬¬{i+1}/{TEST_COUNT}æ¬¡æµ‹è¯•...", end=" ", flush=True)
                elapsed, content, success = test_model(model_id, prompt, max_tokens)

                if success:
                    times.append(elapsed)
                    success_count += 1
                    print(f"âœ“ {elapsed}ms ({elapsed/1000:.2f}ç§’) - è¿”å›{len(content)}å­—ç¬¦")
                else:
                    print(f"âœ— å¤±è´¥: {content}")

            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            scenario_results[model_id] = {
                "times": times,
                "success_count": success_count,
                "total_count": TEST_COUNT,
                "success_rate": (success_count / TEST_COUNT * 100) if TEST_COUNT > 0 else 0
            }

            # æ˜¾ç¤ºç»Ÿè®¡
            if times:
                avg_time = statistics.mean(times)
                min_time = min(times)
                max_time = max(times)
                std_dev = statistics.stdev(times) if len(times) > 1 else 0

                print(f"  â†’ å¹³å‡: {avg_time:.0f}ms ({avg_time/1000:.2f}ç§’)")
                print(f"  â†’ æœ€å¿«: {min_time}ms ({min_time/1000:.2f}ç§’)")
                print(f"  â†’ æœ€æ…¢: {max_time}ms ({max_time/1000:.2f}ç§’)")
                print(f"  â†’ æ ‡å‡†å·®: {std_dev:.0f}ms")
                print(f"  â†’ æˆåŠŸç‡: {scenario_results[model_id]['success_rate']:.1f}%")
            else:
                print(f"  â†’ âŒ æ‰€æœ‰æµ‹è¯•å‡å¤±è´¥")
            print()

        all_results[complexity] = scenario_results

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("=" * 80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 80)
    print()

    for complexity, scenario_results in all_results.items():
        print(f"\nã€{complexity}æç¤ºè¯ã€‘")
        print("-" * 80)

        flash_times = scenario_results["glm-4-flash"]["times"]
        glm47_times = scenario_results["glm-4.7"]["times"]

        if flash_times and glm47_times:
            flash_avg = statistics.mean(flash_times)
            glm47_avg = statistics.mean(glm47_times)

            # è®¡ç®—é€Ÿåº¦å·®å¼‚
            speed_diff = ((glm47_avg - flash_avg) / glm47_avg) * 100
            faster_by = glm47_avg / flash_avg

            print(f"GLM-4-flash å¹³å‡: {flash_avg:.0f}ms ({flash_avg/1000:.2f}ç§’)")
            print(f"GLM-4.7    å¹³å‡: {glm47_avg:.0f}ms ({glm47_avg/1000:.2f}ç§’)")
            print(f"é€Ÿåº¦å·®å¼‚: GLM-4-flash æ¯” GLM-4.7 å¿« {speed_diff:.1f}% ({faster_by:.1f}x)")

            if speed_diff > 50:
                print(f"ğŸ† æ¨è: GLM-4-flash (é€Ÿåº¦å¿«{speed_diff:.0f}%ä»¥ä¸Š)")
            elif speed_diff > 20:
                print(f"ğŸ‘ æ¨è: GLM-4-flash (é€Ÿåº¦æ˜æ˜¾æ›´å¿«)")
            else:
                print(f"âš–ï¸  é€Ÿåº¦æ¥è¿‘ï¼Œå¯æ ¹æ®è´¨é‡éœ€æ±‚é€‰æ‹©")
        else:
            if not flash_times:
                print("âŒ GLM-4-flash æµ‹è¯•å¤±è´¥")
            if not glm47_times:
                print("âŒ GLM-4.7 æµ‹è¯•å¤±è´¥")

        print()

    # æœ€ç»ˆå»ºè®®
    print("=" * 80)
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®")
    print("=" * 80)
    print()

    # è®¡ç®—æ‰€æœ‰åœºæ™¯çš„å¹³å‡é€Ÿåº¦
    all_flash_times = []
    all_glm47_times = []

    for scenario_results in all_results.values():
        all_flash_times.extend(scenario_results["glm-4-flash"]["times"])
        all_glm47_times.extend(scenario_results["glm-4.7"]["times"])

    if all_flash_times and all_glm47_times:
        flash_avg = statistics.mean(all_flash_times)
        glm47_avg = statistics.mean(all_glm47_times)
        speed_diff = ((glm47_avg - flash_avg) / glm47_avg) * 100

        print(f"æ•´ä½“å¹³å‡å“åº”æ—¶é—´:")
        print(f"  â€¢ GLM-4-flash: {flash_avg/1000:.2f}ç§’")
        print(f"  â€¢ GLM-4.7:    {glm47_avg/1000:.2f}ç§’")
        print(f"  â€¢ é€Ÿåº¦å·®å¼‚:   {speed_diff:.1f}%")
        print()

        print("æ¨èä½¿ç”¨åœºæ™¯:")
        print(f"  âš¡ GLM-4-flash - é€‚åˆ:")
        print(f"     â€¢ éœ€è¦å¿«é€Ÿå“åº”çš„åœºæ™¯ (å¦‚å®æ—¶å¯¹è¯)")
        print(f"     â€¢ ç®€å•åˆ°ä¸­ç­‰å¤æ‚åº¦çš„ä»»åŠ¡")
        print(f"     â€¢ å¯¹å“åº”é€Ÿåº¦è¦æ±‚é«˜çš„åº”ç”¨")
        print()

        print(f"  ğŸ§  GLM-4.7 - é€‚åˆ:")
        print(f"     â€¢ å¯¹è¾“å‡ºè´¨é‡è¦æ±‚æé«˜çš„åœºæ™¯")
        print(f"     â€¢ å¤æ‚æ¨ç†å’Œæ·±åº¦åˆ†æä»»åŠ¡")
        print(f"     â€¢ å¯ä»¥æ¥å—è¾ƒé•¿ç­‰å¾…æ—¶é—´çš„åº”ç”¨")
        print()

        if speed_diff > 50:
            print(f"ğŸ¯ å»ºè®®: åœ¨é¥®é£Ÿæ¨èåº”ç”¨ä¸­ä½¿ç”¨ GLM-4-flash")
            print(f"   ç†ç”±: é€Ÿåº¦å¿«{speed_diff:.0f}%ï¼Œç”¨æˆ·ä½“éªŒæ›´å¥½ï¼Œè´¨é‡è¶³å¤Ÿ")
        else:
            print(f"ğŸ¯ å»ºè®®: å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©")
            print(f"   ç†ç”±: é€Ÿåº¦å·®å¼‚è¾ƒå°ï¼Œå¯ä»¥å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡")

    print()
    print("=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 80)


if __name__ == "__main__":
    try:
        run_comprehensive_test()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
